---
title: "Module-17"
author: "Lillian Holden"
date: "2023-11-09"
output: html_document
---
Generalized Linear Models
So far, our discussion of regression centered on standard or “general” linear models that assume normally distributed response variables, normally distributed error terms (residuals) from our fitted models, and constant variance in our response variable across the range of our predictor variables. If these assumptions of general linear regression are not met, we can sometimes transform our variables to meet them, but other times we cannot (e.g., when we have binary or count data for a response variable).

In these cases, however, we can use a different regression technique called generalized linear modeling instead of general linear modeling. Generalized linear models, then, extend traditional regression models to allow the expected value of our response variable to depend on our predictor variable(s) through what is called a link function. It allows the response variable to belong to any of a set of distributions belonging to the “exponential” family (e.g., normal, Poisson, binomial), and it does not require errors (residuals) to be normally distributed. It also does not require homogeneity of variance across the range of predictor variable values, and overdispersion (when the observed variance is larger than what the model assumes) may be present.

One of the most important differences is that in generalized linear modeling we no longer use ordinary least squares to estimate parameter values, but rather use maximum likelihood or Bayesian approaches.

A generalized linear model consists of three components:

The systematic or linear component, which reflects the linear combination of predictor variables in our model. As in general linear regression, these can be be continuous and/or categorical. Interactions between predictors and polynomial functions of predictors can also be included, as in general linear modeling. [Recall that “linear” simply means the regression model is based on linear combinations of regression coefficients, not of variables.]

The error structure or random component, which refers to the probability distribution of the response variable and of the residuals in the response variable after the linear component has been removed. The probability distribution in a GLM must be from the exponential family of probability distributions, which includes the normal (Gaussian), binomial (e.g., if our response variable is binary, like “yes/no”, “presence/absence”), Poisson (e.g., if our reponse variable consists of count data), gamma, negative binomial, etc.

A link function, which links the expected value of the response variable to the predictors. You can think of this as a transformation function. In GLM, our linear component yields a predicted value, but this value is not necessarily the predicted value of our response variable, Y, per se. Rather, the predicted value is some needs to be transformed back into a predicted Y by applying the inverse of the link function.

Common link functions include:

The identity link, which is used to model μ, the mean value of Y and is what we use implicitly in standard linear models.

The log link, which is typically used to model log(λ), the log of the mean value of Y.

The logit link, which is log(π/(1-π)), and is typically used for binary data and logistic regression.



General linear regression can be viewed as a special case of GLM, where the random component of our model has a normal distribution and the link function is the identity link so that we are modeling an expected value for Y.

Model Fitting in Generalized Linear Models
Model fitting and parameter estimation in GLM is commonly done using a maximum likelhood approach, which is an iterative process. To determine the fit of a given model, a GLM evaluates the linear predictor for each value of the response variable, then back-transforms the predicted value into the scale of the Y variable using the inverse of the link function. These predicted values are compared with the observed values of Y. The parameters are then adjusted, and the model is refitted on the transformed scale in an iterative procedure until the fit stops improving. In ML approaches, then, the data are taken as a given, and we are trying to find the most likely model to fit those data. We judge the fit of the model of the basis of how likely the data would be if the model were correct.

The measure of discrepancy used in a GLM to assess the goodness of fit of the model to the data is called the deviance, which we can think of as analogous to the variance in a general linear model. Deviance is defined as 2 times (the log-likelihood of a fully saturated model minus the log-likelihood of the proposed model). The former is a model that fits the data perfectly. Its likelihood is 1 and its log-likelihood is thus 0, so deviance functionally can be calculated at **-2*log-likelihood of the proposed model**. Because the saturated model does not depend on any estimated parameters and has a likelihood of 1, minimizing the deviance for a particular model is the same as maximizing the likelihood. [For the ML process of parameter estimation, it is actually mathematically easier to maximize the log-likelihood, ln(L), than is is to maximize the likelihood, L, so computationally that is what is usually done.] In logistic regression, the sum of squared “deviance residuals” of all the data points is analogous to the sum of squares of the residuals in a standard linear regression.

The glm() function in R can be used for many types of generalized linear modeling, using similar formula notation to that we’ve used before, with an additional argument, family=, to specify the kind of error structure we expect in the response variable (“gaussian”, “binomial”, “poisson”, etc.): glm(y ~ x, family = "gaussian")

As with previous models, our explanatory variable, X, can be continuous (leading to a regression analysis) or categorical (leading to an ANOVA-like procedure called an analysis of deviance) or both.

We will explore two types of GLMs here… logistic regression (used when our response variable is binary) and log-linear or Poisson regression (used when our response variable is count data).

Logistic Regression
As alluded to above, when we have a binary response variable (i.e., a categorical variable with two levels, 0 or 1), we actually are interested in modeling πi, which is the probability that Y equals 1 for a given value of X (xi), rather than μi, the mean value of Y for a given X, which is what we typically model with general linear regression. The usual model we fit to such data is the logistic regression model, which is a nonlinear model with a sigmoidal shape. The error from such a model is not normally distributed but rather has a binomial distribution.

When we do our regression, we actually use as our response variable the natural log of the odds ratio between our two possible outcomes, i.e., the ratio of the probabilities of yi = 1 versus yi = 0 for a given xi, which we call the logit:



where:

πi = the probability that yi equals 1 for a given value of xi and (1−π) = the probability that yi equals 0.
The logit transformation, then, is the link function connecting Y to our predictors. The logit is useful as it converts probabilities, which lie in the range 0 to 1, into the scale of the whole real number line.

We can convert back from a log(odds ratio) to an odds ratio using the inverse of the logit, which is called the expit:



EXAMPLE:

Suppose we are interested in how a students’ GRE scores, grade point averages (GPA), and ranking of their undergraduate institution (into quartiles, 1 to 4, from high to low), affect admission into graduate school. The response variable, “admitted/not admitted”, is a binary variable, scored as 1/0.

Load in the “graddata.csv” dataset, which comes from http://www.ats.ucla.edu/, and then explore it using head(), summary(), pairs(), and table().
```{r}
library(curl)
install.packages("lmtest")
library(lmtest)
```

```{r}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/graddata.csv")
d <- read.csv(f, header = TRUE, sep = ",")
head(d)
```

```{r}
summary(d)
```

```{r}
# first, some exploratory visualization
par(mfrow = c(1, 2))
plot(as.factor(d$admit), d$gpa, xlab = "Admit", ylab = "GPA", col = "lightgreen")
plot(as.factor(d$admit), d$gre, xlab = "Admit", ylab = "GRE", col = "lightblue")
```

```{r}
pairs(d)
```

```{r}
table(d$admit, d$rank)
```

```{r}
# glm of admit~gre
glm <- glm(data = d, admit ~ gre, family = "binomial")
summary(glm)
```

```{r}
x <- seq(from = min(d$gre), to = max(d$gre), length.out = 1000)
logOR <- predict(glm, newdata = data.frame(gre = x))  # this function will predict the log(odds ratio)... but if we add the argument type='response', the predict() function will return the expected response on the scale of the Y variable, i.e., Pr(Y)=1, rather than the odds ratio!
y <- predict(glm, newdata = data.frame(gre = x), type = "response")
plot(d$admit ~ d$gre, pch = 21, type = "p", xlab = "GRE Score", ylab = "Pr(Y)",
    main = "Pr(Y) versus GRE")
lines(y ~ x, type = "l")
```

```{r}
ORchange <- exp(glm$coefficients[2])
ORchange  # a 1 unit increase in gre results in a 0.36% increase in likelihood of admission
```

WALD STATISTIC IS LIKE T VALUE 


```{r}
library(broom)
glmresults <- tidy(glm)
wald <- glmresults$estimate[2]/glmresults$std.error[2]
p <- 2 * (1 - pnorm(wald))  # calculation of 2 tailed p value associated with the Wald statistic
p
```

```{r}
CI <- confint(glm, level = 0.95)  # this function returns a CI based on log-likelihood, an iterative ML process
```

```{r}
CI
```

```{r}
CI <- confint.default(glm, level = 0.95)  # this function returns CIs based on standard errors, the way we have calculated them by hand previously... note the slight difference
CI
```

```{r}
CI <- glmresults$estimate[2] + c(-1, 1) * qnorm(0.975) * glmresults$std.error[2]  # and this is how we have calculated CIs by hand previously
CI
```
### Challenge 1 (NEED TO FINISH THIS CHALLENGE)
Repeat the logistic regression above, but using gpa rather than gre as the predictor variable.

Is gpa a significant predictor of the odds of admission?

What is the estimate of β1 and the 95% CI around that estimate?

How much does an increase of 1 unit in gpa increase the actual odds ratio (as opposed to the log(odds ratio) for admission?

What is the 95% CI around this odds ratio?

HINT: for both of the latter questions, you will need to apply the exp() function and convert the log(odds ratio) to an odds ratio.
Graph the probability of admission, Pr(admit), or πi, for students with GPAs between 2.0 and 4.0 GPAs? HINT: Use the predict() function with type="response" to yield πi directly.
```{r}
glm <- glm(data = d, admit ~ gpa, family = "binomial")
summary(glm)
```

```{r}
coeffs <- glm$coefficients
coeffs
```

```{r}
CI <- confint(glm, level = 0.95)
```

```{r}
CI
```

```{r}
ORchange <- exp(coeffs[2])
ORchange
```

```{r}
ORchangeCI <- exp(CI[2, ])
ORchangeCI
```

```{r}
library(ggplot2)
x <- data.frame(gpa = seq(from = 2, to = 4, length.out = 100))
prediction <- cbind(gpa = x, response = predict(glm, newdata = x, type = "response"))
# IMPORTANT: Using type='response' returns predictions on the scale of our
# Y variable, in this case Pr(admit); using the default for type would
# return a prediction on the logit scale, i.e., the log(odds ratio), or
# log(Pr(admit)/(1-Pr(admit)))
head(prediction)
```

```{r}
p <- ggplot(prediction, aes(x = gpa, y = response)) + geom_line() + xlab("GPA") +
    ylab("Pr(admit)")
p
```

```{r}
prediction <- cbind(gpa = x, predict(glm, newdata = x, type = "response", se = TRUE))
prediction$LL <- prediction$fit - 1.96 * prediction$se.fit
prediction$UL <- prediction$fit + 1.96 * prediction$se.fit
head(prediction)
```

```{r}
p <- ggplot(prediction, aes(x = gpa, y = fit))
p <- p + geom_ribbon(aes(ymin = LL, ymax = UL), alpha = 0.2) + geom_line() +
    xlab("GPA") + ylab("Pr(admit)")
p <- p + geom_point(data = d, aes(x = gpa, y = admit))
p
```
### Likelihood Ratio Tests
```{r}
glm1 <- glm(data = d, admit ~ 1, family = "binomial")
glm2 <- glm(data = d, admit ~ gpa, family = "binomial")
anova(glm1, glm2, test = "Chisq")
```


```{r}
library(lmtest)
```

```{r}
lrtest(glm1, glm2)
```

```{r}
Dglm1 <- glm1$deviance  # intercept only model
Dglm1
```

```{r}
Dglm1 <- deviance(glm1)
Dglm1
```

```{r}
Dglm2 <- glm2$deviance  # model with intercept and one predictor
Dglm2
```

```{r}
Dglm2 <- deviance(glm2)
Dglm2
```

```{r}
chisq <- Dglm1 - Dglm2  # this is a measure of how much the fit improves by adding in the predictor
chisq
```

```{r}
p <- 1 - pchisq(chisq, df = 1)  # df = difference in number of parameters in the full verus reduced model
p
```

```{r}
x2 <- glm1$null.deviance - glm1$deviance
x2  # why is this 0? because glm1 *is* the intercept only model!
```

```{r}
p <- 1 - pchisq(x2, df = 1)
p
```

```{r}
x2 <- glm2$null.deviance - glm2$deviance
x2
```

```{r}
p <- 1 - pchisq(x2, df = 1)  # df = difference in number of parameters in the full verus reduced model
p
```

#Challenge 2
Using the same “graddata.csv” dataset, run a multiple logistic regression analysis using gpa, gre, and rank to look at student admissions to graduate school. Do not, at first, include interaction terms.

What variables are significant predictors of the log(odds ratio) of admission?

What is the value of the log(odds ratio) coefficient and the 95% CIs around that value for the two continuous variable (gpa and gre), when taking the effects of the other and of rank into account? What do these translate into on the actual odds ratio scale?

Is the model including all three predictors better than models that include just two predictors?

Compare a model that includes the three predictors with no interactions versus one that includes the three predictors and all possible interactions.
```{r}
d$rank <- as.factor(d$rank)  # make sure rank is a categorical variable
glmGGR <- glm(data = d, formula = admit ~ gpa + gre + rank, family = binomial)  # 3 predictor model
summary(glmGGR)
```

```{r}
coeff <- glmGGR$coefficients  # extract coefficients... all significantly different from 0
coeffCI <- cbind(coeff, confint(glmGGR))  # and 95% CIs around them... none include 0
```

```{r}
coeffCI
```

```{r}
ORcoeff <- exp(coeff)
ORcoeff
```

```{r}
ORcoeffCI <- exp(coeffCI)
ORcoeffCI
```

```{r}
# Compare 2 verus 3 factor models
glmGG <- glm(data = d, formula = admit ~ gpa + gre, family = binomial)
glmGR <- glm(data = d, formula = admit ~ gpa + rank, family = binomial)
glmRG <- glm(data = d, formula = admit ~ gre + rank, family = binomial)
anova(glmGG, glmGGR, test = "Chisq")
```

```{r}
anova(glmGR, glmGGR, test = "Chisq")
```

```{r}
anova(glmRG, glmGGR, test = "Chisq")
```

```{r}
# Compare model with and model without interactions
glmNO <- glm(data = d, admit ~ rank + gpa + gre, family = "binomial")
glmALL <- glm(data = d, admit ~ rank * gpa * gre, family = "binomial")
anova(glmNO, glmALL, test = "Chisq")  # adding interaction terms to model doesn't significantly decrease deviance
```
## Log-Linear or Poisson Regression
```{r}
library(curl)
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/woollydata.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = TRUE)
head(d)
```

```{r}
summary(d)
```

```{r}
# first, some exploratory visualization
par(mfrow = c(1, 1))
p <- ggplot(data = d, aes(x = age, y = success)) + geom_point() + xlab("Age") +
    ylab("Mating Success")
p
```

```{r}
pairs(d)
```

```{r}
table(d$rank, d$success)
```

```{r}
# glm of success~age
glm <- glm(data = d, success ~ age, family = "poisson")
summary(glm)
```

```{r}
coeffs <- glm$coefficients
coeffs
```

```{r}
CIs <- confint(glm, level = 0.95)  # uses ML approaches
```

```{r}
CIs
```

```{r}
CIs <- confint(glm, level = 0.95)  # uses standard errors
```

```{r}
CIs
```

```{r}
x <- data.frame(age = seq(from = 5, to = 17, length.out = 30))
prediction <- cbind(age = x, predict(glm, newdata = x, type = "response", se = TRUE))
# IMPORTANT: Using the argument type='response' makes our prediction be
# units of our actual Y variable (success) rather than log(success)
prediction$LL <- prediction$fit - 1.96 * prediction$se.fit
prediction$UL <- prediction$fit + 1.96 * prediction$se.fit
head(prediction)
```

```{r}
p <- p + geom_line(data = prediction, aes(x = age, y = fit)) + geom_ribbon(data = prediction,
    aes(x = age, y = fit, ymin = LL, ymax = UL), alpha = 0.2) + xlab("Age") +
    ylab("Mating Success")
p  # note the curvilinear 'line' of best fit
```

```{r}
glm1 <- glm(data = d, success ~ 1, family = "poisson")
glm2 <- glm(data = d, success ~ age, family = "poisson")
# using the anova function
anova(glm1, glm2, test = "Chisq")
```

```{r}
# based on the deviance between a specified null and full models
x2 <- glm1$deviance - glm2$deviance
x2
```

```{r}
p <- 1 - pchisq(x2, df = 1)
p
```

```{r}
# based on hand calculating deviance for each model; logLik() function
# returns the log-likelihood of a model
Dglm1 = -2 * logLik(glm1)
Dglm1
```

```{r}
Dglm2 = -2 * logLik(glm2)
Dglm2
```

```{r}
x2 <- as.numeric(Dglm1 - Dglm2)
x2
```

```{r}
p <- 1 - pchisq(x2, df = 1)  # df = difference in number of parameters in the full verus reduced model
p
```
As mentioned briefly in Module 16, the Akaike Information Criterion, or AIC, is another way of evaluating and comparing related models. For similar models, those with lower AIC models are preferred over those with higher AIC. The AIC value is based on the deviance associated with the model, but it penalizes model complexity. Much like an adjusted R-squared, it’s intent is to prevent you from including irrelevant predictors when choosing among similar models. Models with low AICs represent a better fit to the data, and if many models have similarly low AICs, you should choose the one with the fewest model terms. For both continuous and categorical predictors, we prefer comparing full and reduced models against one another to test individual terms rather than comparing the fit of all possible models to try and select the “best” one. Thus, AIC values are useful for comparing models, but they are not interpretable on their own. The logLik() function returns the log-likelihood associated with a particular model and can be used to calculate AIC values by hand.
```{r}
AIC <- 2 * 2 - 2 * logLik(glm2)  # formula for AIC = 2 * # params estimated - 2 * log-likelihood of model; for thise model we estimated 2 params
AIC
```

```{r}
AICreduced <- 2 * 1 - 2 * logLik(glm1)  # for this model, 1 param is estimated
AICreduced
```
## Challenge 3
Using the woolly monkey mating success data set, explore multiple Poisson regression models of [a] mating success in relation to rank and [b] mating success in relation to age + rank (and their interaction) on your own. What conclusions can you come to about the importance of rank and rank in combination with age versus age alone?
```{r}
# glm of success~age
glm1 <- glm(data = d, success ~ rank, family = "poisson")
summary(glm1)
```

```{r}
coeffs <- glm1$coefficients
coeffs
```

```{r}
CIs <- confint(glm1, level = 0.95)
```

```{r}
CIs
```

```{r}
# glm of success~age+rank
glm2 <- glm(data = d, success ~ age + rank, family = "poisson")
summary(glm2)
```

```{r}
coeffs <- glm2$coefficients
coeffs
```

```{r}
CIs <- confint(glm2, level = 0.95)
```

```{r}
CIs
```

```{r}
# glm of success~age+rank+age:rank
glm3 <- glm(data = d, success ~ age * rank, family = "poisson")
summary(glm3)
```

```{r}
coeffs <- glm3$coefficients
coeffs
```

```{r}
CIs <- confint(glm3, level = 0.95)
```

```{r}
CIs
```
