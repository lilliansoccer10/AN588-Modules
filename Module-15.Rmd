---
title: "Module 15"
author: "Lillian Holden"
date: "2023-11-02"
output: html_document
---

```{r}
R = matrix(cbind(1, 0.8, -0.5, 0, 0.8, 1, -0.3, 0.3, -0.5, -0.3, 1, 0.6, 0,
    0.3, 0.6, 1), nrow = 4)
```

```{r}
n <- 1000
k <- 4
M <- NULL
V <- NULL
mu <- c(15, 40, 5, 23)  # vector of variable means
s <- c(5, 20, 4, 15)  # vector of variable SDs
for (i in 1:k) {
    V <- rnorm(n, mu[i], s[i])
    M <- cbind(M, V)
}
M <- matrix(M, nrow = n, ncol = k)
orig <- as.data.frame(M)
names(orig) = c("Y", "X1", "X2", "X3")
head(orig)
```

```{r}
cor(orig)  # variables are uncorrelated
```

```{r}
plot(orig)  # does quick bivariate plots for each pair of variables; using `pairs(orig)` would do the same
```

```{r}
ms <- apply(orig, 2, FUN = "mean")  # returns a vector of means, where we are taking this across dimension 2 of the array 'orig'
ms
```

```{r}
sds <- apply(orig, 2, FUN = "sd")
sds
```

```{r}
normalized <- sweep(orig, 2, STATS = ms, FUN = "-")  # 2nd dimension is columns, removing array of means, function = subtract
normalized <- sweep(normalized, 2, STATS = sds, FUN = "/")  # 2nd dimension is columns, scaling by array of sds, function = divide
head(normalized)  # now a dataframe of Z scores
```

```{r}
M <- as.matrix(normalized)  # redefine M as our matrix of normalized variables
```

```{r}
U = chol(R)
newM = M %*% U
new = as.data.frame(newM)
names(new) = c("Y", "X1", "X2", "X3")
cor(new)  # note that is correlation matrix is what we are aiming for!
```

```{r}
plot(orig)
```

```{r}
plot(new)  # note the axis scales; using `pairs(new)` would plot the same
```

```{r}
df <- sweep(new, 2, STATS = sds, FUN = "*")  # scale back out to original mean...
df <- sweep(df, 2, STATS = ms, FUN = "+")  # and standard deviation
head(df)
```

```{r}
cor(df)
```

```{r}
plot(df)  # note the change to the axis scales; using `pairs(d)` would produce the same plot
```

### Challenge 1
Start off by making some bivariate scatterplots in {ggplot2}. Then, using simple linear regression as implemented with lm(), how does the response variable (Y) vary with each predictor variable (X1, X2, X3)? Are the β1 coefficients significant? How much of the variation in Y does each predictor explain in a simple bivariate linear model?
```{r}
library(ggplot2)
require(gridExtra)
```

```{r}
g1 <- ggplot(data = df, aes(x = X1, y = Y)) + geom_point() + geom_smooth(method = "lm",
    formula = y ~ x)
g2 <- ggplot(data = df, aes(x = X2, y = Y)) + geom_point() + geom_smooth(method = "lm",
    formula = y ~ x)
g3 <- ggplot(data = df, aes(x = X3, y = Y)) + geom_point() + geom_smooth(method = "lm",
    formula = y ~ x)
grid.arrange(g1, g2, g3, ncol = 3)
```

```{r}
m1 <- lm(data = df, formula = Y ~ X1)
summary(m1)
```

```{r}
m2 <- lm(data = df, formula = Y ~ X2)
summary(m2)
```

```{r}
m3 <- lm(data = df, formula = Y ~ X3)
summary(m3)
```

```{r}
m <- lm(data = df, formula = Y ~ X1 + X2 + X3)
coef(m)
```

```{r}
summary(m)
```

```{r}
# let's check if our residuals are random normal...
plot(fitted(m), residuals(m))
```

```{r}
hist(residuals(m))
```

```{r}
qqnorm(residuals(m))
```

```{r}
f <- (summary(m)$r.squared * (nrow(df) - (ncol(df) - 1) - 1))/((1 - summary(m)$r.squared) *
    (ncol(df) - 1))
f
```
### Challenge 2
Load up the “zombies.csv” dataset again and run a linear model of height as a function of both weight and age. Is the overall model significant? Are both predictor variables significant when the other is controlled for?
```{r}
library(curl)
```

```{r}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/zombies.csv")
z <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = TRUE)
head(z)
```

```{r}
m <- lm(data = z, height ~ weight + age)
summary(m)
```
### ANCOVA
```{r}
library(car)
```

```{r}
m <- lm(data = z, formula = height ~ gender + age)
summary(m)
```

```{r}
m.aov <- Anova(m, type = "II")
m.aov
```

```{r}
plot(fitted(m), residuals(m))
```

```{r}
hist(residuals(m))
```

```{r}
qqnorm(residuals(m))
```

How do we interpret these results?

The omnibus F test is significant

Both predictors are significant

Controlling for age, being male adds 4 inches to predicted height when compared to being female.

Visualizing the Model

We can write two equations for the relationship between height on the one hand and age and gender on the other:

For females, height = 46.7251 + 0.94091 x age

In this case, females are the first level of our “gender” factor and thus do not have additional regression coefficients associated with them.
For males, height = 46.7251 + 4.00224 + 0.94091 x age

Here, the additional 4.00224 added to the intercept term is the coefficient associated with genderMale
```{r}
library(ggplot2)
p <- ggplot(data = z, aes(x = age, y = height)) + geom_point(aes(color = factor(gender))) +
    scale_color_manual(values = c("goldenrod", "blue"))
p <- p + geom_abline(slope = m$coefficients[3], intercept = m$coefficients[1],
    color = "goldenrod4")
p <- p + geom_abline(slope = m$coefficients[3], intercept = m$coefficients[1] +
    m$coefficients[2], color = "darkblue")
p
```

```{r}
m <- lm(data = z, formula = height ~ age + gender)
summary(m)
```

```{r}
confint(m, level = 0.95)
```
### Challenge 3
What is the estimated mean height, in inches, for a 29 year old male who has survived the zombie apocalypse?

What is the 95% confidence interval around this mean height?

What is the 95% prediction interval for the individual heights of 29 year old male survivors?
```{r}
ci <- predict(m, newdata = data.frame(age = 29, gender = "Male"), interval = "confidence",
    level = 0.95)
ci
```

```{r}
pi <- predict(m, newdata = data.frame(age = 29, gender = "Male"), interval = "prediction",
    level = 0.95)
pi
```

```{r}
m <- lm(data = z, height ~ age + gender + age:gender)  # or
summary(m)
```

```{r}
m <- lm(data = z, height ~ age * gender)
summary(m)
```

```{r}
coefficients(m)
```
Here, when we allow an interaction, there is no main effect of gender, but there is an interaction effect of gender and age.

If we want to visualize this…

female height = 48.1817041 + 0.8891281 * age

male height = 0.481704 + 1.1597481 + 0.8891281 * age + 0.1417928 * age
```{r}
library(ggplot2)
library(gridExtra)
p1 <- ggplot(data = z, aes(x = age, y = height)) + geom_point(aes(color = factor(gender))) +
    scale_color_manual(values = c("goldenrod", "blue"))
p1 <- p1 + geom_abline(slope = m$coefficients[2], intercept = m$coefficients[1],
    color = "goldenrod4")
p1 <- p1 + geom_abline(slope = m$coefficients[2] + m$coefficients[4], intercept = m$coefficients[1] +
    m$coefficients[3], color = "darkblue")
p1
```

```{r}
p2 <- ggplot(data = z, aes(x = age, y = height)) + geom_point(aes(color = factor(gender))) +
    scale_color_manual(values = c("goldenrod", "blue")) + geom_smooth(method = "lm",
    aes(color = factor(gender), fullrange = TRUE))
grid.arrange(p1, p2, ncol = 2)
```
### Challenge 4

Load in the “KamilarAndCooper.csv”" dataset we have used previously

Reduce the dataset to the following variables: Family, Brain_Size_Female_Mean, Body_mass_female_mean, MeanGroupSize, DayLength_km, HomeRange_km2, and Move

Fit a Model I least squares multiple linear regression model using log(HomeRange_km2) as the response variable and log(Body_mass_female_mean), log(Brain_Size_Female_Mean), MeanGroupSize, and Move as predictor variables, and view a model summary.
```{r}
library(dplyr)
```

```{r}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = TRUE)
head(d)
```

```{r}
d <- select(d, Brain_Size_Female_Mean, Family, Body_mass_female_mean, MeanGroupSize,
    DayLength_km, HomeRange_km2, Move)
```

Look at and interpret the estimated regression coefficients for the fitted model and interpret. Are any of them statistically significant? What can you infer about the relationship between the response and predictors?

Report and interpret the coefficient of determination and the outcome of the omnibus F test.

Examine the residuals… are they normally distributed?

What happens if you remove the “Move” term?

```{r}
m <- lm(data = d, log(HomeRange_km2) ~ log(Body_mass_female_mean) + log(Brain_Size_Female_Mean) +
    MeanGroupSize + Move)
summary(m)
```

```{r}
plot(m$residuals)
```

```{r}
qqnorm(m$residuals)
```

```{r}
shapiro.test(m$residuals)
```

```{r}
m <- lm(data = d, log(HomeRange_km2) ~ log(Body_mass_female_mean) + log(Brain_Size_Female_Mean) +
    MeanGroupSize)
summary(m)
```

```{r}
plot(m$residuals)
```

```{r}
qqnorm(m$residuals)
```

```{r}
shapiro.test(m$residuals)  # no significant deviation from normal
```

